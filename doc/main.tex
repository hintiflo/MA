%%% File encoding: UTF-8
%%% äöüÄÖÜß  <-- no German umlauts here? Use an UTF-8 compatible editor!

%%% Magic comments for setting the correct parameters in compatible IDEs
% !TeX encoding = utf8
% !TeX program = pdflatex 
% !TeX spellcheck = de_DE
% !BIB program = biber

\documentclass[master,english,smartquotes,apa]{hgbthesis}
% Valid options in [..]: 
%    Type of work: 'diploma', 'master' (default), 'bachelor', 'internship' 
%    Main language: 'german' (default), 'english'
%    Turn on smart quote handling: 'smartquotes'
%    APA bibliography style: 'apa'
%%%-----------------------------------------------------------------------------

\RequirePackage[utf8]{inputenc} % Remove when using lualatex or xelatex!

\graphicspath{{images/}}  % Location of images and graphics
\logofile{logo}           % Logo file: images/logo.pdf (no logo: \logofile{})
\bibliography{references} % Biblatex bibliography file (references.bib)

% \usepackage[table]{xcolor}
\usepackage{longtable}

%%%-----------------------------------------------------------------------------
% Title page entries
%%%-----------------------------------------------------------------------------

\title{ Application of Software Quality Measures to Bare-metal Firmware for Optical Coherence Tomography}
\author{Florian Hinterleitner}
\programname{embedded systems design}
\programtype{Fachhochschul-Masterstudiengang}
\placeofstudy{Hagenberg}
\dateofsubmission{2022}{06}{15} % {YYYY}{MM}{DD}
\advisor{Langer, Rankl, Zorin} % optional
%\strictlicense % restrictive license instead of Creative Commons (discouraged!)
\definecolor{gray}{gray}{.80}

\input{_newcomms.tex}

\begin{document}
\frontmatter                                   % Front part (roman page numbers)
\maketitle

\tableofcontents

% \include{front/preface} % A preface is optional
\include{front/abstract}		
\include{front/kurzfassung}			
\mainmatter                                    % Main part (arabic page numbers)
\include{chapters/introduction}
\include{chapters/fundamentals}
% \include{chapters/_fundamentals}

	
	\chapter{Requirements}
	\label{cha:Requirements}
		% Allgemeine REQs an FW, RT (und evtl design-for-testability)
		% \section{Firmware-Requirements}
		\TODO{Formulieren}
		Two meetings, including all stakeholders of the OCTane, resulted in all requirements towards the OCTane and especially the firmware running on it. As these are imposed by the stakeholders, or, in other words, by the users of the OCTane, they are called 'user requirements'. Accompanying tags in the form 'RU-xx' allow for tracing relations between a requirement and according test-cases or points of implementation inside the source-code.
			\section{User requirements}
				\input{src/_REQS.tex}
		\pagebreak
			\section{USB-Protocol}
		SCPI Commands can be in 'short form', defined by the capital letters, or in 'long form', defined by the whole string. OCTane accepts both forms as commands and is case-insensitive. \cite{scpi1993}

			{	\scriptsize
				\input{src/_scpiRecendt.tex}
			}
			Table ~\ref{USB-Protocol-responses} specifies the responses by the OCTane, if they are not described sufficiently in the previous table.

			{	\scriptsize
				\input{src/_scpiStd.tex}
			}
			
	\subsection{Analogue outputs Resolution and LSB}
	mapping 20Vpp Voltage space to a resolution of 16bit
	\begin{itemize}
		\item 0 ... 30000 ... 60000
		\item 1000 ... 31000 ... 61000
		\item 0 ... 32767 ... 65535
		\item ???
	\end{itemize}
	$\rightarrow$ LSB \^{=} ...mV

			\subsection{Load-Hypothesis, Fault-Hypothesis}
			\subsection{Traceability-Matrix}
			Linking Requirements by there tags, to the SW-modules, where they are fulfilled
			The traceability Matrix establishes the relations between user requirements and test cases. Furthermore it is good practice to also include requirement IDs inside the source code on the exact point of implementation. The convenient layout for a traceability matrix is to list requirement IDs column-wise, while noting the IDs of the test cases row wise. The reason being that one requirement may have multiple test cases and and it is more convenient to note these multiple IDs in rows, rather than columns.
			\subsection{V-Model}
			\subsection{Test-cases}
			Automated test-cases are the primary method to ensure code-quality, because they allow for the assessment of functionalities against requirements and produce according documentation of conducted tests. Furthermore, they help identifying  errors in case of failures and secure implemented functionalities during future adaptions. To demonstrate a complete assessment of the firmware, for every existing user-requirement, at least one test-case is necessary. Depending on the depth of examination through the layers of a firmware, test-cases can either be external or internal. External test-cases denote code running on a separate device, controlling and probing the device under test, most useful for a behavioural and user-oriented verification. Internal test-cases are part of the firmware itself, allowing inspection of hardly accessible areas of code. Though running on the target-hardware, these tests also need to transfer test-data onto an external device, capable of storing and report-generation. Best-practice is to apply both approaches according to necessity and even combine them into hybrid test-cases: External code providing input and capturing output, while calling internal test-cases, that can access and deliver information of the device usually inaccessible via production code.
			
			For practical reasons, python-scripts, running on a proxy host-device, controlling the device under test via USB implement these test-cases. The first practical aspect, being, that the resulting test-data are directly available on a device with capable processing power and high memory capacity, facilitating the automated generation of test-reports. The python programming language provides the package 'pytest-html', allowing for the automated generation of test-reports in HTML-format. Every test-case has to be simple enough, to render verification of the test-code itself unnecessary. A test-case, so complex, that i would require a superordinate test-case, indicates, that said test-case should be split into several cases of lesser respective complexity. Fig. ~\ref{testCaseExample} contains an exemplary test-case, assessing the devices correct reply to an identification query. \cite{BalajiScpi}
			
			\bildGr{h!}{testCaseExample}{testCaseExample}{testCaseExample}{0.75\textwidth}
			The general procedure of this test-case consists of 
			\begin{itemize} \setlength\itemsep{1px}
			\item Sending a command to the device under test,
			\item Gathering resulting test-data and
			\item Verification of retrieved data against requirements.
			\end{itemize}
			Upon execution via 'pytest-html', this test-case either results as 'passed' or 'failed', depending on the contained assertions. Furthermore, it causes an entry in the resulting report-file, likewise to the extract in fig. ~\ref{reportExample}
			
			\bildGr{h!}{reportExample}{reportExample}{reportExample}{0.75\textwidth}
			
			In case of commands resulting in digital, analogue or serial output-signals, these signals shall be automatically measured as part of a test-case. Automated measurement is required if performable with justifiable effort and available measurement instruments. Apart from this method of fully automated testing, few requirements demand assessment in a manual fashion. For example, oscillograms of the resulting analogue signals require evaluation by the eye of a skilled engineer. Automation of this process via spectral analysis or automated comparison with reference signals would require unjustifiable effort, compared to an evaluation via visual inspection. \\
			Test-cases, usually, belong to one of the following classes, \TODO{according to the V-Model}:
			\subsection{Unit-Tests}
			Unit-tests are test-cases that evaluate the correctness of single functions, methods or procedures. A single variable, array or data-set also constitutes such a unit, if the contained data demands deliberate assessment via a test-case. Viable inputs exist in the form of binary values with separate cases for 'true' and 'false'. In case of numerical input, be they of integer or floating-point nature, the boundary-value and equivalence-class methods deliver suitable input values. For inputs in text form, all specified valid texts, and at least one invalid text form a set of useful input values. \cite{jorgensen13}
			\subsection{Integration-Tests}
			The next level of tests concern the interactions between units and their correct cooperation to form correctly working modules and sub-systems. A major focus in integration testing lies on the verification of units and modules to ensure their correct interaction. As this aspect is of a lesser concern during unit-testing, integration-testing is an established branch of verification in its own right. Furthermore, side-effects of units, which are hardly a concern during unit-testing, are important aspects during integration-testing. Testing and demonstrating seamless interoperability and collaboration of modules are the prime objectives. \cite{SpilSoft2005} % \TODO{welche inputs? Oder: beschreibung der integrations-Art nach ISTQB S. 56} Similar to a unit-test for text-driven functions, an integration test has to contain all possible valid inputs and at least one invalid. \\
			The strategy of integration, happening during implementation has significant influence on the design of suitable integration-tests. These are the most common approaches:
			\begin{itemize} \setlength\itemsep{1px}
			\item Top-down integration 
			\item Bottom-up integration 
			\item Ad-hoc integration 
			\item Backbone integration 
			\end{itemize} 
			
			\subsection{System-Tests}
			% https://de.wikipedia.org/wiki/Heisenbug
			TODO{kann ma den da reinschummeln?}
			\cite{Beizer95}
			\subsection{Load/Fault Tests}
			
			\TODO{cite: IEEE830.pdf, Crowder-REQs, Buttazzo, system-deadlocks (Coffman) und Datenblätter }
			\subsection{End to end Test}
			The core concern is if a test is intended and designed as a unit or an end to end test, regardless of additional resources of the system being used.
			Even though unit tests are not into end-to-end tests they may very well use the outer interfaces of the system under test.

			\subsection{Code Coverage}
			Code Coverage is an accompanying metric to test-cases, indicating their accuracy and completeness of assessment. The goal for this project is to achieve complete statement and branch coverage for the original, self-written source-code. Third-party libraries and HAL-modules from the processor-vendor are excluded from this requirement.
			
	\chapter{Concept}
	\label{cha:Concept}
		\section{ System Architecture }
		% \subsection{Modules of the Firmware}
		\bildGr{h!}{_FW-Modules}{Basic Modular concept}{_FW-Modules}{\textwidth}
		The modular concept for the firmware in fig. \ref{_FW-Modules} contains all major modules and there dependencies. the 'MAIN'-module merely performs initializations and starts up the finite state machine contained in the 'FSM'-module. This component handles the domain-logic, incoming scpi-messages, generates scpi-replies and programs instances of the 'Trigger'-module to generate signal-sequences. Furthermore, the 'FSM'-module receives and transmits scpi-messages via the 'USB'-component, a set of library function provided by the vendor of the microprocessor. Also, interactions with low-level-functions, contained in the 'HAL'-module, are managed by the 'FSM'-module. The 'Trigger'-module has control over the analogue output-sources to produce the actual signal-voltages and the 'Timers'-module for correct frequencies and synchronization. This image contains a basic architecture and acts as a 'big picture', an overview of the whole firmware. Further details would create an indistinct image, or are not yet decided during this design phase.

		\section{Module Integration Strategy}
			To ensure an order for the development of the modules and provide an organizational frame, a suitable integration strategy is necessary. These strategies constitute the most promising approaches in module integration: \\
			\begin{itemize} \setlength\itemsep{1px}
			\item Top-down integration 
			\item Bottom-up integration 
			\item Ad-hoc integration 
			\item Backbone integration \\
			\end{itemize} 

			Top-down integration approaches the code under test from its entry point and external interfaces, non-existing sub-systems require replacement through stubs delivering dummy-data. \\
			
			Bottom-up integration builds modules and subs-systems based upon existing low-level functions, while non-existing higher-ranking systems require replacement through test-drivers delivering dummy-commands. \\
			
			Ad-hoc integration is the least formal integration-strategy, where components undergo integration directly after integration, regardless of their level or rank within the complete system. The effort in planning integration is negligible, while non-existing components demand higher effort for their replacements. Furthermore, the lack of a thoroughly planned integration phase might diffuse into the resulting software exhibiting an erratic and patchy structure. \\
			
			Backbone integration \cite{Beizer90} is the formal equivalent of ad-hoc integration, where the initial task is, to build an overarching backbone or skeleton for the whole project and afterwards integrate components in arbitrary order. This leaves substantial freedom to the order of developing components, alas requires thorough planning up front and notably effort initially, to implement the backbone. \\

			Backbone integration is the most promising option and consequently the selected choice for the project at hand. The appeal of that strategy stems from the possibility to develop and integrate components in any order. This allows to work on another component, if one imposes seemingly unsolvable problems and come back to that problematic component at a later point. In contrast to the ad-hoc method, backbone integration maintains order and structure over a software project, while leaving mentioned degrees of freedom. \\

		\section{Layer Structure}
		Combining the chosen strategy with the modular concept from fig. \ref{_FW-Modules} and taking more detailed requirements into account, leads to the refined layer-structure in fig. \ref{_sysArch}.
		\bildGr{h!}{_sysArch}{layer structure}{_sysArch}{\textwidth}

		It extends the basic system-architecture and adds further sub-modules. The 'Main'-module incorporates the handling of Errors and Resource-Management. As this module is already responsible with the initialization of the system, adding Resource-Management to the same module is a sensible decision. The finite state machine sits at the heart of the architecture, as it handles commands and responses, performs parametrization of other components and request the start of signal-sequences from the triggers. Therefore, the separate graphic ~\ref{FSMbig} contains the even more detailed implications of the state machine. \\
		
		The necessity to measure timings and synchronization between modules with external instruments, suggests to introduce a Debug-Unit. This module allows control over eight separate digital outputs, to signal activity of up to eight functions, events or exceptions. All dependencies with other modules are ad-hoc and temporary, except for calls to the 'HAL'-module to access the digital outputs. Therefore the figure depicts no further dependencies of the debug-unit, as they are entirely optional. \\
		
		The 'Triggers'-module exists in three instances for three separate signal-sequences, that perform either individually, or inter-dependent. Fig. ~\ref{_Octane_Trigger_extended_neato.pdf} illustrates this matter in detail. the core matter of triggers is to parametrize and start timer-instances and therefore initiate signal-sequences. Per default, timers deactivate themselves after certain counts, but in a so-called 'infinite'-mode, it is also upon the superior trigger to stop signal-generation. Further responsibilities are to establish inter-dependencies between timers and reception of trigger-events via digital inputs, buttons or from the state-machine. \\
		
		Timers accept instructions by their respective triggers and in turn have control over their outputs. Two timers control analogue output sources and use trigger-outputs to signal their respective states. While the third timer has only a trigger-output, but no analogue source. Upon activation, a timer performs a given number of trigger-events at certain frequency and causes its signal-source to output the correct voltage according to the counter-index, while generating a rectangular pulse on the according trigger-output. After completion of one such sequence, the timer deactivates itself, unless the 'infinite'-mode is active, where it performs the described sequence on repeat. \\
		
		Two instances of the 'Sources'-module control analogue output voltages via a serial peripheral interface (SPI) and access these via the HAL. Source-instances handle the signal-vectors holding the digital representation of the analogue signals, so, the samples to be issued via SPI. Aside from access to the vectors samples, sources also provide functionality to load a default-signal into vectors, manipulate the vectors by scaling or shifting, or completely overwrite them with arbitrary signals. Additionally, enabling of the analogue outputs is under control of the corresponding source-module, again, via HAL. Triggers have primary control over the source-instances. A reach-through from the state machine to the sources happens for certain operational modes, but is not part of the figure, because of minor significance and to preserve a transparent layer structure. \\
		
		The original parts of the USB-module wrap the according components from the vendor and manage message-strings between the FSM and the USB-port. \\
		
		A Miscellaneous-module provides cyclic-redundancy-checking (CRC) for secure transfer of commands and replies. The microprocessor contains an onboard CRC-peripheral and the Misc-module makes use of this resource. A second responsibility of this module is to handle built-in watchdog functionality. \\
		
		The hardware abstraction layer (HAL) utilises the following resources provided by the processor:
		\begin{itemize} \setlength\itemsep{1px}
		% \item general purpose inputs/outputs (GPIO) as digital 
		\item serial peripheral interface (SPI)
		\item digital inputs/outputs (DIO)
		\item universal asynchronous receiver transmitter (UART)
		\item inter-integrated communication (I2C)
		\item analogue inputs (AIN)
		\end{itemize}
		It provides functionalities to write and read mentioned resources as well as initialize and de-initialize them. 
		
			Modules-Skizze + HW-Graph und ein meta-graph der diese verbindet.

		\section{FSM}
		\bildGr{h!}{../src/_mainFSM_neato.pdf}{Overarching Finite state machine}{FSMbig}{0.75\textwidth}
		The overarching finite state machine of the firmware adheres to the state chart in fig.	~\ref{FSMbig}. It contains states as yellow boxes and arrowheads indicating possible transitions. Nest to arrows are the according conditions for every transition, while a '1' denotes an unconditional transition. \\
		
		Beginning from the entry point, the state-machine enters an 'init'-state, performing high-level initializations, mostly, setting the default parameters for signal-generation. \\
		
		It further proceeds to an 'idling'-state, where it is ready to receive scpi-commands by the USB-module. In this case, the 'rxUsb'-state takes over, buffering the incoming messages and returns to the 'idling'-state. In case of new parameters, the 'param'-state handles the correct application of parameters to the according sub-modules. A possible reply, in form of an outgoing USB-message, causes activation of the 'txUSB'-state, as long as messages are enqueued for transmission via the USB-module. \\
		
		In case of an 'arm'-command, the according 'armed'-state becomes active, if all preconditions are met. This requires, that all resources for signal-generation are available and hold valid parameters, otherwise the FSM falls back into the 'idling'-state and reports the denial of the 'arm'-command. \\
		
		Upon successfully reaching the arm command, only the commands 'idle', 'off' or 'run' are accepted. While the first two send the FSM back into 'idling'-state, the latter causes the start of signal-generation. This suspends USB-connectivity, unless 'infinite'-mode is active and, most important starts all necessary triggers. The subsequent 'running'-state is active, during the actual signal-generation and per default, accepts no USB-commands. As soon as all triggers are finished and inactive again, or, in 'infinite'-mode, via an 'idle'- or 'off'-command, the FSM lands in 'idling'-mode. It also resumes full USB-connectivity. \\
		
		The FSM enters an 'errorState' upon detection of serious failures, requiring a restart of the complete device. The errorState deactivates the USB-connection completely and initiates the actual restart. The reason for the USB-disconnect is, to allow a smooth re-connection after the restart. The errorState does not send any final message, to prevent babbling idiot failures \cite{Wang2009AvoidingTB}. \\
		
		A 'restart'-command, received via USB, also causes a complete restart of the device. A 'reset'-command sends the FSM into the 'init'-state, merely resetting all parameters for signal-generation and return to 'idling'-state afterwards. \\
		
		\subsubsection{Triggers and Voltage - Outputs}
		association of Triggers and their analogue outputs
		\begin{itemize}
			\item TriggerB $\rightarrow$ SourceB $\rightarrow$ Vout1
			\item TriggerA $\rightarrow$ SourceA $\rightarrow$ Vout2
		\end{itemize}

		\bildGr{h!}{../src/_Octane_Trigger_basic.pdf}{basic Trigger-concept}{_Octane_Trigger_extended_neato}{0.35\textwidth}
		\bildGr{h!}{../src/_Octane_Trigger_extended_neato.pdf}{extended Trigger-concept}{_Octane_Trigger_extended_neato}{0.75\textwidth}


		\subsection{Standard operation procedures (SOP)}
		{	\scriptsize
			\input{src/_SOPs.tex}
		}

		\section{Hardware}
			\subsection{STM32F4}
			\bildGr{h!}{../src/_Octane_HW-Structure.pdf}{HardWare}{_Octane_HW-Structure.pdf}{0.75\textwidth}
			
			\subsection{Wandler, Level-Shifter, HighSider}
			\subsection{Connection of Galvos and Triggers}
				\begin{table}[h!]
					 \begin{tabular}{|p{5.5cm}|p{6cm}|} \hline
					Source1	- Galvo y (slow)& Trigger B\\ \hline
					Source2	- Galvo x (fast)& Trigger A\\ \hline
					 \end{tabular}
					 \caption{Assignment of Triggers and according analogue outputs}
				\end{table}

		
		
	\chapter{Implementation}
	\label{cha:Implementation}
		\section{Trigger-Diagramme}
		\section{Timer usage}
			\begin{itemize} \setlength\itemsep{1px}
			\item 3 capture compare timers for signal-generation
			\item 1 timer basic for kex debouncing 
			\item 1 timer basic for reading timeouts
			\item 1 timer basic for flashing LEDs
			\end{itemize}
		\subsection{Trigger-Lines and Timers}
		utilisation of the output compare - timers
		\begin{itemize}
			\item TrigA \^{=} $TRIG\_2$  \^{=} PB3 $\leftarrow$ $TIM2_CH2$
			\item TrigB \^{=} $EN\_3$    \^{=} PC6 $\leftarrow$ $TIM8_CH1$
			\item TrigC \^{=} $EN\_4$    \^{=} PC7 $\leftarrow$ $TIM3_CH2$
		\end{itemize}
			% 3 Timers necessary, old concept: double frequency and on modulo 2 will be decided if PinSet and DACwrite, or PinClear
			% new conspt: output compare Timer etiher the advanceds from the F4 for TrigB and C with separate ISRs to set and clear
						% OR three GP-Triggers with dedicated output lines, that are set/reset by Timer itself (PSC, ARR and pulse)
						% $\rightarrow$ see schematic wich Trigger has wich line and Graph against according Timers!
		\subsection{Debug-Unit}
		A debug-unit, offering eight digital outputs via set.. and rst.. - functions, was established. Fig. ~\ref{dbgUnitLogic} shows a 'ladder' setting and resetting all debug-Pins upon initialization of the module.
		
			\bildGr{H}{dbgUnitLogic.png}{dbgUnitLogic}{dbgUnitLogic}{0.5\textwidth}

		
		
		\section{Verification}
				Regarding verification of the firmware, the decision fell on behavioural assessment via external tests on the one hand and internal tests on the other hand. A controlling host-computer performs external tests by issuing commands to the device under test via USB and reading back replies and measuring results on according outputs, as far as feasible. Internal tests are part of the firmware itself and support verification of code-parts not accessible to external/behavioural tests.
			\subsection{External Testing with pytest-html}
				'pytest-html' is a practical python-package to verify python-code. The ability to send and receive data via USB, automatically execute test-cases and generate according reports as html-file, also renders it an ideal tool to examine the device under test at hand. Every test-case focusses on a set of commands from the devices complete USB-protocol, sends a sequence of these commands to the device under test and reads back the replies. The comparison of sent sequence and the according replies and assessment against the devices requirements leads to the decision, if a test-case passes or fails. In particular, assertions in the testing python-code state, which command should cause which reply, causing tests to pass or fail. Every single result of a test-case is an entry in a html-report including result, a meaningful name for every case and excerpt records of the communication between host and device. In case of a failing test, an outline of the failing assertion is part of the report as well. \\
				If a USB-command causes a measurable physical output on the device under test, the according test-case should measure that output and take it into account the test result. Efforts in programming according tests and limited availability in suitable instruments might render certain automated test-cases unfeasible time-consuming. In these cases, human intervention by manual inspection of measurement instruments and documentation of the results would be more feasible. The core purpose of this project is developing a firmware and not an elaborate test-setup. \\
				\TODO{Bilder und FW-Bezug aus den REQs hierher holen und nur den simplesten test-case dort beschreiben} The primary intention of pytest-html is to examine python code, therefore, some adaptions are necessary for the examination of an external device: \\
				\begin{itemize} \setlength\itemsep{1px}
				\item establishing a USB-connection with the device under test
				\item instructing the device under test for correct level of verbosity
				\item formatting incoming replies 
					\begin{itemize} \setlength\itemsep{1px}
						\item encoding byte-streams into strings
						\item removing line-breaks
						\item trimming of surplus spaces, tabs and indention-symbols
					\end{itemize}
				\item correct opposition of excitation-commands and expected results for assertions \\
				\end{itemize}
			Especially, the last task is of interest: On a desktop-system, the assertion of a test typically compares equality of the output of a unit under test with expected results. In the actual situation, the assertion examines, if a certain input provokes the correct corresponding output, instead of just comparing two values for equality. 
			
			\subsection{Coverage Measurement with Gcov}
				'gcov' is a suitable software tool to measure code coverage and part of the gnu compiler collection 'gcc'. It is free of charge, open source and operates on various platforms like Linux, Windows and Mac and supports target platforms ranging from arm microprocessors to x86 processors. It is able to perform statement as well as branch coverage analysis. While the actual GCC compiler produces instrumented code suitable for coverage measurement, gcov merely converts coverage data post factum into human readable reports.
			\subsubsection{Instrumentation}
				The production code per se is not suited for coverage measurement, because no data is generated to base coverage analysis on. Therefore it is necessary to insert counter variables into the source code that keep track on how often every block of code was executed and which branches of decisions were taken and which not. Furthermore data management of those counters is necessary as well as exporting generated data onto a host system for further processing. Instrumentation is the process of inserting mentioned counters and the according data management into the original source code. Additionally the compiler generates information about basic blocks, arcs and information to relate them to line numbers in the source code. Basic blocks are sequences of statements without any branching statements and branch targets. Arcs on the other hand are branch operations and their according targets that link basic blocks together. \\
				
				The instrumented source code is not identical to production source code! In case of the gcov software tool the instrumentation is done by the actual GCC compiler and not the gcov tool itself. To force the compiler to produce instrumented code, the flags '-fprofile-arcs' and '–ftest-coverage' are necessary. The instrumented code is merely an intermediate by-product, not directly visible to the developer. It is good practice, to generate instrumented code only for one module at a time and not a complete project at once, especially on a bare-metal firmware-project. They binary footprint of the instrumented source-code is substantially larger than the non-instrumented code. Therefore, the demand in memory might overwhelm the limited resources of the embedded processor, if attempting to measure coverage for a complete project at once. Post-processing tools for gcov allow for assembling data-sets of several modules into one report. With the mentioned flags, the compiler measures how often a program reaches a branch instruction and how often it actually performs a branch operation. For this purpose GCC creates a control flow graph and an accompanying spanning tree. Statements not contained in this spanning tree require counting, as they are not executed in every case. \\
				
				
			\subsubsection{Producing Raw Data}
				Executing instrumented code produces a 'gcda'-file, holding information about control-flow-graph and the spanning tree. By default, the instrumenting instructions rely on syscalls to generate and fill files with the gathered counter-data. This step requires a workaround on bare-metal systems, as there, the mentioned syscalls only exist as dummies. A subsequent chapter describes this workaround. Now it is known which branch operations the program performed and how often it executed  each statement and block of code. This information allows to write to generate a 'gcov' file and a statistical analysis. The 'gcov' file is essentially the original source code, which the gcov-tool prepends with annotations about execution-counts of statements, branching information and omitted statements, retrieved from the binary gcda-file. \\

				The '-g' compiler option might be helpful for measurement of coverage data, even though it is not mandatory. It forces the compiler to generate debug information in the executable binary. This allows for example to perform breaks during execution of an instrumented program. This is a useful option for simulating failed memory allocations. Executing statements that attempt to access memory that was faulty allocated demonstrates how a program handles faulty memory regions. It is an exceptional situation for the program and therefore complicated to provoke during test cycles. Therefore, simulating exceptions via the debuggers break-functionality can be a useful method to test handling of said exceptions. So, to achieve complete branch coverage it, might be necessary to incorporate the GDB-debugger into the process. This thesis omits a detailed explanation of this debugger, as it is not the core topic. \cite{gcov} \\
				
				To produce the raw coverage information, at least one execution of the instrumental program is necessary. Repeated execution of the program results in the instrumenting code parts to append additional counter information to the already existing gcda file. This step again requires a certain amount of work around on bare metal systems.
			\subsubsection{Processing Raw Data}
				After one or several executions of the instrumented program, gcov is able to link and analyse the information contained in the gcda-file the gcno-file and the original source code. This results in the mentioned gcov-file and statistic data about the source-codes coverage. By applying the '-b' flag upon execution of gcov, branch coverage information is generated as well. Typically gcov states the percentages of executed code lines, of executed branches, of taken branches and of executed function-calls, in one module. The '-c' option allows to retrieve statistics about branching operations in absolute values rather than percentages. The '-f' option delivers statement coverage also for every separate function, additionally to the statement coverage of a whole module.
				% \TODO{include coverage example laying around somewhere in my system and screenshots of this source code and the report file}
			\subsubsection{Post-processing}
				Up to this point the existing analysis data is already of valuable insight for a developer, but scattered among modules and not in a presentable format. This calls for post-processing via tools like 'lcov', 'llvm-cov', 'kcov' or 'gcovr', who are able to generate an aggregate report of several modules in HTML-, JSON-, CSV-, and XML-format. \cite{lcov} \cite{kcov}
				Fig. ~\ref{gcovReportDebugUnit01} depicts an extract of the coverage report of one submodule and ~\ref{gcovReport01} contains a composite report about code coverage over the whole firmware-project. As the figures originate from an early phase of the project, they contain only one module called 'DebugUnit'. The software tool 'gcovr' produces such expressive reports as HTML-files. The summarizing statistics, the coverage metrics appear on the right side of the header of every report, with a colouring-scheme similar to traffic-lights, to indicate problematic metrics. The total report replicates this concept, itemized for every single module in the body part of the output. Reports of separate modules list the according source-code in the body part, indicating covered code-portions in green and not executed parts in red. This representation provides an intuitive overview of areas, that require further testing.
			\bildGr{H}{gcovReport01}{gcovReport01}{gcovReport01}{0.75\textwidth}
			\bildGr{H}{gcovReportDebugUnit01}{gcovReportDebugUnit01}{gcovReportDebugUnit01}{0.75\textwidth}
			
			% Although not a direct part of the firmware the basic steps to measure Code coverage on the bear Metal System are described here. 
			\subsection{Measuring Code Coverage on bare-metal systems}
			Code coverage imposes a significant challenge on bare metal systems: Measurement-tools assume an underlying operating system, especially a file-system, that receives the measured raw data. This requires a workaround, as 'bare-metal' literally means 'without an operating system'. It is very-well possible to establish a file-system, alas at significant demands in processing power and memory. As these resources require economical utilization, transferring data directly out of the device under test during test-cycles is an appealing alternative. Most ARM-based processors provide ITMs, instrumentation trace macro-cells, coupled with SWO (serial wire output), a dedicated debug-interface. This allows to define debug-informations in firmware and provide them via SWO. Debug-probes, like ST-Link2, support this interface and pass the data on to a host-system. The processors serial interfaces like USB or RS-232 are unaffected by this and remain available for user-application, instead of debugging-purposes. \\
			
			Fig. ~\ref{coverageFlow} illustrates the process, applying the described components for coverage measurement: A compiler generates executable binary-code from the instrumented code under test, a programming tool transfers the compiled code to the target platform. As gcov utilises the syscalls \lstC ! _open() ! , \lstC ! _write() ! and  \lstC ! _exit() !, additional code redirects these calls to the processors ITM-functions. During test-cycles, the IT-macrocell provides the emerging coverage information via SWO, while a debug-probe picks up this information. The probe then passes the information on to a host-PC, capable of storing large volumes of data and further processing. An ad-hoc-script, written in python, converts the raw coverag information into gcda-files, compatible with gcov and its derivates. Either a combination of gcov and lcov, or gcovr then leads to a final HTML-report, similar to fig. ~ref{gcovReportDebugUnit01}. \\
			\bildGr{H}{coverageFlow}{coverageFlow}{coverageFlow}{0.95\textwidth}
			Special attention regarding versions of the compiler, the coverage tool and the reporting tool is necessary. Report generation easily fails, if the versions of the tools generating gcno-, gcda- and html-files do not match up exactly. Furthermore, only one exact version (v5) of the arm-compiler generates instrumented source code applicable for coverage measurement on STM32 processors. Lcov does not produce any reports containing any coverage metrics at all, at least not with the compiler- and gcov-versions in use. \\
			\bildGr{h!}{gcvor52}{gcvor52}{gcvor52}{0.5\textwidth}
			Unfortunately, only versions up to 4.2 of gcovr provides compatibility among compiler, coverage-tool and the platform performing the coverage analysis. While the most recent version 5.2 delivers separate metrics for function-calls and decisions (see fig. ~\ref{gcvor52}), version 4.2 is only capable of providing statement and branch-coverage\cite{gcovr} . \\

			The described process is rather cumbersome, hardly automated and requires some manual intervention. Nevertheless, the resulting reports are of significant value to the developer, which again justifies the efforts. 

		\section{\GREY{Software tools}}
			\subsection{\GREY{CubeIDE}}
			\subsection{\GREY{python tools, pytest-html}}

			% \subsection{\GREY{OpenOCD}}
			% \subsection{\GREY{Valgrind}}
			% \subsection{\GREY{Wavedrom}}
			% \subsection{\GREY{WireShark/USBPcap}}
			% \subsection{\GREY{Gitlab Runner}}
			% \subsection{\GREY{HIL-Setup}}
	
	\chapter{Results}
	\label{cha:Results}
		% Results
	\section{Measurements}
		Messaufbau, werte, ergebnisse, interpretation
		\subsection{Oszi, Debug-Unit und Opto-Detektoren}
		\section{Test-Res}
		\section{Test-Cases}
		\section{Code Coverage}
		\section{Code Review}
		\section{Review Remarks}
		\section{Gavlo-Performance}
		\section{Project-status}
		\section{ ... }
	% thethesis.tex
	\chapter{Conclusion}
	\label{cha:Conclusion}
	\section{Test Cases}
		\bildGr{h!}{exampleHelpfulFailedTest01}{exampleHelpfulFailedTest01}{exampleHelpfulFailedTest01}{0.85\textwidth}
	Fig. ~\ref{exampleHelpfulFailedTest01} contains an example of a helpful test-case that failed in the beginning. The direct comparison of expected and actual results allows to backtrack the problem via searching for the unexpected result-string. This leads to the accompanying enum-ID of that string, that was misplaced because of a typing error of one single letter. 
	The Fig. ~\ref{exampleHelpfulFailedTestSrc01} depicts the according place in the source code, with the erroneous ID \lstC !TX_SOURB_VOLT_LEVq! instead of \lstC !TX_SOURB_VOLT_LEV! .
		\bildGr{h!}{exampleHelpfulFailedTestSrc01}{exampleHelpfulFailedTestSrc01}{exampleHelpfulFailedTestSrc01}{0.55\textwidth}
	This example highlights the advantages of numerous simple test-cases, already during the debugging and early verification of newly implemented functionalities.
	

% \include{chapters/figures}
% \include{chapters/mathematics}
% \include{chapters/literature}
% \include{chapters/galvoChar}
% \include{chapters/closing}

%%%-----------------------------------------------------------------------------
\appendix                                                             % Appendix 
%%%-----------------------------------------------------------------------------

% \include{back/appendix_a} % Technical supplements
\include{back/appendix_b} % Contents of the CD-ROM/DVD
% \include{back/appendix_c} % Chronological list of changes
% \include{back/appendix_d} % Source text of this document

%%%-----------------------------------------------------------------------------
\backmatter                           % Back part (bibliography, glossary, etc.)
%%%-----------------------------------------------------------------------------

\include{back/_abbrev}

\MakeBibliography % References

%%%-----------------------------------------------------------------------------
% Special page for checking print size
%%%-----------------------------------------------------------------------------

% \include{back/printbox}

%%%-----------------------------------------------------------------------------
\end{document}
%%%-----------------------------------------------------------------------------

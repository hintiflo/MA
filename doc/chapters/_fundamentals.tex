	% nicht von Jasmin gelesen ...
	
	\subsection{statement coverage test}
	% Characteristics and Objectives of Statement Coverage Testing
	The statement coverage test is the simplest control flow oriented test method. It is also referred to as the $C_0$ test for short. The aim of statement coverage is to execute all statements of the program to be tested at least once, i.e. to cover all nodes of the control flow graph. The degree of statement coverage achieved is defined as a test measure. It is the ratio of the executed instructions to the total number of instructions in the test object.
		\begin{align*}
		C_0 = \frac{\textrm{number of executed statements}}{\textrm{number of statements}}
		\end{align*}
	If all statements of the module to be tested have been executed at least once by the entered test data, then complete statement coverage is achieved. The strategy of executing all statements that a programmer has assembled into a program at least once with test data is immediately obvious. It ensures that there are no instructions in the software under test that have never been executed. On the one hand, the execution of a statement is certainly a necessary criterion that must be met in order to find an error contained in the statement. On the other hand, this is not a sufficient criterion, as the occurrence of the error effect can be dependent to the execution with certain input data. 
	% Suppose a software module contains the erroneous decision (x > 5). The correct decision should be (x 5). The erroneous decision can be made with any value of x except the value 5 without erroneous behavior occurring. The erroneous and the correct decision behave identically for all values with the exception of the value 5. Consequently, simply executing the faulty point does not guarantee that a fault will occur and that the fault will be recognized.
	
	Even if the misconduct has occurred, it cannot be guaranteed that it will be recognized by the testing person. The erroneous situation must propagate to an externally observable point. The statement coverage test tries to fulfil the necessary criterion of the execution of potentially faulty parts of the software. The execution of all instructions is part of almost all important test procedures, which also take other aspects into account. As an independent test procedure, the statement coverage test takes a subordinate position and is directly supported by only a few tools. The statement coverage test offers the possibility of detecting non-executable statements, so-called 'dead code'. The statement coverage measure can be used to quantify the achieved test coverage. Statement coverage testing is rarely the main function of testing tools. It occurs as a by-product of tools supporting branch coverage testing and other testing tools. The statement coverage test is considered to be too weak a criterion for meaningful test execution. It is of minor practical importance.

	The minimum criterion of the control flow-oriented test techniques is the so-called branch coverage test, which contains the statement coverage test. It is therefore not advisable to use the applications coverage test alone as a structure-oriented test completeness criterion.

	\subsection{branch coverage test}
	Branch coverage testing is a more rigorous testing technique than statement coverage testing. The statement coverage test is fully contained, or: subsumed, in the branch coverage test. The branch coverage test is generally considered to be the minimum criterion in the area of control flow-oriented testing. It is also referred to as the $C_1$ test for short.
		\begin{align*}
		C_1 = \frac{\textrm{number of executed branches}}{\textrm{number of branches}}
		\end{align*}
	The goal of branch coverage testing is to execute all branches of the program under test. This requires running through all edges of the control flow graph. The ratio of the number of executed branches to the number of branches present in the software under test is usually used as a simple measure for branch coverage. The central position of the branch coverage test is particularly illustrated by its position within the test procedure. As a necessary test technique, it is subsumed by most other test procedures \TODO{(Bild mit Test-Arten)}. The branch coverage test forms the largest common subset of all of these testing techniques. The branch coverage test provides the ability to detect non-executable program branches. This is the case when no test data can be generated that causes the execution of a branch that has not yet been run through. Software parts that are run through particularly often can be identified and specifically optimized. \\
	In some cases it may be difficult to run all branches of the program for various reasons. Often unexecuted branches would be executable but the test cases are difficult to generate, for example, because operating system states or file constellations cannot be created with justifiable effort, or the test cases are difficult to derive from the program itself. In principle, branches can also not be executable. This would be a design error that resulted in an unnecessary branch. \\

	As with the statement coverage test, it also applies to the branch coverage test, that software with a coverage rate of 100% does not necessarily have to be error-free. Neither combinations of branches nor complex decisions are taken into account. Also, loops are not tested sufficiently. A single pass through the loop body of blocking loops and a repeat of non-blocking loops is sufficient for branch coverage. In connection with the testing of loops, it is particularly critical that an arbitrary chosen number of loop repetitions is executed to test their correct behaviour. More powerful control flow-oriented test techniques provide extensions at these points and try to check loops more appropriately, take account of the dependencies between branches or analyze composite decisions more closely. \\

	The simple branch coverage measure proves to be problematic. Since all branches are weighted equally without considering dependencies between them, there is no linear relationship between the achieved coverage rate and the ratio between the number of test cases required. The uncritical use of the simple coverage measure leads to an overestimation of the test activities carried out, since the coverage rate is always greater than the number of test cases carried out so far in relation to the total number of test cases with complete coverage of all branches. \\

	The reason is that each test case executes a sequence of branches, some of which generally do not belong to just one path, but are part of multiple paths and thus multiple test cases. The test cases performed at the beginning of the test already cover a large number of these branches. A relatively high coverage rate is achieved after a few test runs. The remaining test cases also cover these branches, but only increase the coverage rate by executing the branches not contained in the already tested paths. A relatively large number of test cases are therefore required at the end of the test in order to achieve a small increase in the coverage rate. \\

	One approach to solving this problem considers the execution dependencies between branches as a criterion for influencing the degree of coverage. A branch is not considered if it is executed whenever another branch is executed. The branches that do not have this property are called primitive or essential. Since the execution of the primitive branches ensures the execution of all non-primitive branches due to the dependency described, it is sufficient to use only the primitive branches for the calculation of the coverage measure.
	\begin{align*}
		C_{primitive} = \frac{\textrm{number of executed primitive branches}}{\textrm{number of primitive branches}}
	\end{align*}
	C\textsubscript{primitive} gives a more linear relationship between coverage and number of test cases than the simple measure of coverage. 
	% \GREY{In addition, only some of the branches have to be instrumented, which correspondingly reduces the effort caused by the instrumentation.} \\

	The branch coverage test is the minimum criterion of structure-oriented software testing, a superset of the statement coveage and even prescribed by various safety standards. \\
	A variety of supporting tools for different programming languages exist for the branch coverage test. Such tools usually work instrumentally. The tool analyses the control structure of the software module present in the source code, locates branches and inserts additional instructions (counters) allowing to trace the control flow. If branches are not represented by corresponding statements in the program text, e.g. B. if the optional else construct of a branch is not used, a corresponding statement is generated by the tool. This so-called instrumented version of the DUT is compiled and the generated executable program is executed with test data. The information collected by the added instructions during the test runs can then be evaluated. Additional test cases must be created for branches that are not run through. In addition, the tool can display the degree of branch coverage achieved. Some tools offer the possibility of displaying the branches executed by the test case entered last, which is used in particular to check the control flow, and keep overall statistics to identify branches that have not been executed and program parts that have been run through particularly frequently. \\
	Because of the importance of branch coverage testing as a necessary test criterion and the excellent availability of appropriate testing tools, the use this technique is highly advisable. Whoever, this should be done with tool support, as carrying out structure-oriented tests by hand becomes unnecessary cumbersome.

	\subsection{Condition Coverage Test}

	The condition coverage test considers the logical structure of decisions of the software under test. Different forms exist, the weakest of which - the simple condition coverage test - does not subsume the statement and branch coverage test! The so-called multiple condition coverage test subsumes branch coverage, but has other weaknesses that will be discussed in more detail below. The minimal multiple condition coverage test and the so-called modified condition/decision coverage test represent a feasible middle ground.

	The basic idea of condition coverage tests is the thorough examination of composite decisions of the software under test. For a complete branch coverage test it is sufficient that the evaluation of all decisions delivers the value true and false once. This strategy does not take into account the often complicated structure of the decisions, which often contain nested logical links of partial decisions on several levels.

	In the general case, it cannot be guaranteed that the simple condition coverage test subsumes the branch coverage test. Whether the branch coverage test is subsumed determines the way decisions are evaluated. If compound decisions are implemented by the compiler in such a way that they are incompletely evaluated when the software is run, the branch coverage test is included in the simple condition coverage test. This way of evaluating decisions is the standard case. Since in this case composite decisions are only checked until the truth value of the overall decision is known, this form of decision evaluation reduces the execution time.

	In a full evaluation of the decisions, the branch coverage test is not included in the simple condition coverage test. Some compilers offer the choice of whether decisions should be fully or partially evaluated. Since branch coverage testing is considered a necessary testing technique, and it is certainly unacceptable if the fulfillment of necessary criteria depends on the compiler used or its settings, the simple condition coverage test must usually be considered insufficient.

	\subsection{Condition/Decision Coverage Test}

	The condition/decision coverage test guarantees full branch coverage testing in addition to simple condition coverage. It explicitly requires branch coverage to be established in addition to Condition coverage. Since the simple condition coverage test already ensures this in the case of an incomplete evaluation of decisions, this technique is only relevant in the case of a complete evaluation of decisions. % Executing test cases 5 and 12 according to Table 3-5 results in complete condition/decision coverage, since the partial decisions A, B, C and D and the overall decision are each evaluated as true and false. Tab. 3-5 shows that this is possible without the combined ?partial decisions (A || B) and (C || D) being checked against both truth values. The partial decision (A || B) has the value true in both test cases. The conditional/decision coverage test checks atomic sub-decisions and overall decisions. However, he largely ignores the logical structure of complicated decisions on several levels. The condition/decision coverage test does not contain any requirements for the test of composite partial decisions below the overall decision.


	\subsection{Minimum Multiple Condition Coverage Test}
	The minimum multiple condition coverage test requires that, in addition to the atomic sub-decisions and the overall decision, all composite sub-decisions are also checked against true and false. This technique subsumes the condition/decision coverage test. Since decisions can be hierarchically structured, it makes sense to take this structure into account when testing. The minimum multiple condition coverage test requires that all decisions - regardless of whether they are atomic or not, are tested against both truth values. This form of condition coverage takes the structure of decisions into account better than the techniques presented above, since all nesting levels of a complicated decision are considered equally.

	% With a complete evaluation ...
	% If the decision was erroneously ((A && B) | | (C && D)), this test case would have run differently. The sub-decisions A, C, (A && B) and (C && D) would have been evaluated incorrectly. The sub-decisions B and D would not have been evaluated. The overall decision is wrong. You get the same result but in a different way. The evaluation of the decision is aborted at other points, which offers a chance to detect the errors.
	% Tab. 3-8 shows the test cases and truth values of the minimal multiple condition coverage test with complete evaluation of the decisions of ZaehleZchn.
	% Tab. 3-9 assumes an incomplete decision evaluation. Considering e.g. E.g. decision b) of ZaehleZchn, in the case of an evaluation from left to right, the partial decision furthest to the right is only evaluated if the evaluation of the other decisions gave the wrong value. The reason for this behaviour is the OR linkage of the partial decisions. Since an OR link already yields the value true if one of the linked partial decisions is true, the evaluation is terminated when a true partial decision is discovered. The rightmost atomic sub-decision can only be evaluated incorrectly if all other atomic sub-decisions have also been evaluated incorrectly. This means that the OR-linked overall condition also gives the value false. The requirement to cover all atomic parts| decisions consequently leads to incomplete decision evaluation| Iuation also to cover the non-atomic partial decisions and the overall decision.

	The requirement to cover all partial decisions makes sense, since it must apply to every atomic or non-atomic decision that it can assume both truth values. If no test cases can be generated for a partial decision that would cause it to assume a truth value that has not yet been tested, then it is invariant. In this case, the decision can be equivalently transformed so that the corresponding partial decision is omitted. The invariant partial decisions correspond to the non-executable branches of the branch coverage test or the non-executable statements of the statement coverage test. They can be removed and indicate a software error.

	On the one hand, the minimal multiple condition coverage test subsumes the branch coverage test. He considers the logical structure of decisions and identifies invariant partial decisions. On the other hand, it is only partially able to recognize erroneous logical operators, especially in the case of the complete evaluation of decisions.

	\subsection{Modified Condition/Decision coverage test}

	The modified condition/decision coverage test requires test cases that demonstrate that each atomic sub-decision can affect the truth value of the overall decision independently of the other sub-decisions. In other words, the technology aims to test the logic of composite decisions as comprehensively as possible with a reasonable test effort. The relationship between the number of atomic sub-decisions of a decision and the number of required test cases is linear. At least n+1 test cases are required to test a decision with n sub-decisions.



	The modified condition/decision coverage test subsumes the minimum multiple condition coverage test. In the incomplete evaluation of decisions, the branch coverage test at the object code level and the minimal multiple condition coverage test at the source code level correspond to each other.

	% Extensions of the modified condition/decision coverage test have been proposed for coupled sub-decisions /Chilenski, Miller 94/. These are required because certain ?truth value combinations may not be generated. An example of a decision with coupled partial decisions is ((Zchn == A) | | (Zchn == E) | | (Zchn == I) | | (Zchn == O) | | (Zchn == U)) of Operation CountInc. It is not possible to change the truth value of a sub-decision completely independently of the values of the other sub-decisions because all sub-decisions refer to the value of the same variable. Changing the truth value of a partial decision can, but does not necessarily have to, have an impact on the truth values of the other partial decisions. Such atomic partial decisions are called weakly coupled. In the given example it is not possible to get more than one partial decision to be true at the same time. This can lead to difficulties in generating the test cases required for a full modified constraint/decision coverage test. However, the test can often be carried out regardless of the coupled conditions.

	% In addition to the weakly coupled atomic partial decisions described, there are so-called strongly coupled atomic partial decisions. These always change their truth value if the truth value of one of the coupled partial decisions changes. ?For the implementation of a modified condition/decision coverage test for strongly coupled atomic sub-decisions /Chilenski, Miller 94/ propose appropriate extensions of the procedure.

	\subsubsection{Multiple Condition Coverage Test}

	The multiple condition coverage test requires the testing of all truth value combinations of the atomic sub-decisions. This approach undoubtedly yields a very comprehensive test of composite decisions. In addition, when all possible combinations are taken into account, it is ensured that both truth values are taken into account for the overall decision, regardless of the linking logic of composite decisions. The multiple condition coverage test therefore in any case subsumes the branch coverage test and all other condition coverage testing techniques. The disadvantage is its high testing effort. A decision made up of n sub-decisions always requires $2^n$ test cases. This is referred to as exponential growth of the test effort. Such an exponential growth in the number of test cases - and thus the test effort - is usually unacceptable, except for a very small amount of sub-divisions. 

	% In the case of an incomplete evaluation of decisions, not all 16 test cases exist. Only the seven situations given in Tab. 3-16 can be created. In principle, of course, test data can be generated for the 16 combinations of truth values. However, there is no way to use a test tool to register situations other than the seven given in Tab. 3-16 if the evaluation of decisions is incomplete. A possibly possible switchover of the compiler to a complete evaluation of decisions should not be used, since the test item should show as few differences as possible to the later released and delivered software. If an incomplete evaluation of decisions is provided there, it should not be any different for the test candidate. ?In addition, it happens that certain combinations of truth values cannot be produced due to coupled partial decisions. Of the 32 (25) truth value combinations of the decision ((Zchn == 4â?T) || Zchn == â?~Eâ?T) | | (Zchn == T') | | (Zchn ==â?~0') | | (Zchn == â?~Uâ?T)) of the operation ZaehleZchn only six can be produced (Table 3-17). This does not indicate an error in the program logic, but is only natural in terms of the properties of the variable used in the decision. This makes it difficult to define a meaningful test measure. The objective of test measures is to obtain a quantitative statement about the degree of the test. The quotient of the number of tested objects (instructions, branches, atomic partial decisions) and the number of objects that are assumed to be testable are usually formed here. Since some of the required tests often cannot be carried out in the multiple condition coverage test, a simple measure of the form described is not permitted.

	% 3.4.7 Issues

	% The type of evaluation of decisions has a significant influence on the possible test cases of the condition coverage tests. In principle, there is a significant difference between whether decisions are fully or partially reviewed. In addition, there are many possibilities for the incomplete examination of decisions. In the above examples, an incomplete left-to-right evaluation was implicitly assumed. But that is by no means the only possibility. So e.g. B. Optimizing compilers for efficiency reasons significantly redesign compound decision. ?Because microprocessors do not have the ability to evaluate complicated decisions, the compiler converts them into nested structures with atomic decisions. In this way, branches arise in the object code for the atomic partial decisions, which can serve to register condition coverages. Appropriate tools insert statements at these points that register during test execution if the branch is run through. Adding these statements is called instrumentation. The condition coverage achieved can therefore be registered in the object code in a simple manner.

	% In the incomplete decision evaluation assumed here from left to right, the logical AND operation of the Boolean expressions is converted by the compiler into a structure in the object code, the form of which is described by the source code representation given in the example. Two problems arise:

	% If BooleVar already yields the truth value false, BooleProc(x) is no longer evaluated due to the nesting structure. This procedure is correct because only the truth value of the overall decision has to be determined and it is already clear that this has the truth value false. The multiple condition coverage test can therefore not always be implemented in the object code, since partial decisions are often not evaluated. There is no way to register specific combinations of truth values of atomic partial decisions.

	% An explicit evaluation of partial decisions is forbidden, since Boolean expressions can also contain function calls. If their planned call sequence is changed, incorrect reactions in the case of memory-bound functions are the result. There is also a risk that z. B. due to index errors in fields runtime errors ?occur. In any case, the instrumented software behaves differently than the non-instrumented version, which is unacceptable.

	% Several approaches are conceivable to solve the problem:

	% One possibility is the implementation of control constructs with compound decisions in nested structures of elementary decisions at the source code level, which can then be instrumented. This causes an extensive change in the control structure of the examinee. In addition, the method assumes that the type of evaluation of decisions by the compiler is known.

	% Another possibility is to only evaluate those Boolean expressions that would also be evaluated in the non-instrumented version of the test item. The truth values of the expressions can be assigned to additional Boolean variables. These variables are used analogous to the decisions of the original program to control the further control flow. This leaves the control constructs largely in their original state. Assumptions about how complex decisions are implemented in nested structures of atomic sub-decisions at the object code level are still necessary.

	% A better way to solve the problem is to directly instrument within the decisions using a Boolean function. This form of instrumentation does not require any assumptions about how complicated decisions are implemented in nested structures of atomic sub-decisions at the object code level. It leaves the control structure completely unaffected. No additional Boolean variables are required, and the decisions only need to be slightly modified. Each atomic and non-atomic sub-decision is replaced by a call to a Boolean function that has the sub-decision as its current parameter. The function registers the logical value of its current parameter and returns it as the function value. The structure of the decision remains unchanged. Therefore, the compiler translates the instrumented version analogously to the non-instrumented version of the decision. The evaluation and registration of the truth value of partial decisions takes place at runtime in a completely equivalent way to the non-instrumented version of the program.

	% 3.4.8 Evaluation of the condition coverage test

	The condition coverage tests are particularly interesting as testing techniques when there is complicated processing logic that leads to complicated decisions. In terms of the best compromise between performance and testing effort, the minimum multiple condition coverage test and the modified condition/decision coverage test are recommended.

	% ... nicht von Jasmin gelesen
	\subsection{Techniques for testing loops}

	% 3.5.1 Characteristics and Objectives

	Loops often cause an extremely high number of program paths, theoretically one for each repetition, if a counting variable is involved. The execution of these paths is not feasible in this case. A solution to this problem is provided through structured path tests and boundary interior coverage methods. These approaches divide paths into 'equivalence classes' and only execute appropriate proxies from those classes of paths. The two techniques are closely related, the boundary interior test is a special case of the structured path test. On the one hand, the techniques in the primary literature are not described with sufficient precision, so that in the case of complicated loop structures it is not entirely clear which requirements have to be met. On the other hand, the aim of these techniques is to define a test criterion for loops that can be carried out with reasonable effort and that complies with certain rules. Thus, one will require that a full branch coverage test be achieved as a constraint, because we are looking for a testing technique that sits between the branch coverage test and the path coverage test. Depending on the underlying definition of the process, there are loop structures for which one of the requirements mentioned is not met. The structured path test as a third method has both reasonable execution effort, as well as sufficient coverage.


	\subsubsection{Structured path test and boundary interior path test}

	The number of different paths of a software module can become extremely high in the presence of loops, one for each repetition. However, this does not apply to every type of loop. Counting loops with a constant number of repetitions do not pose a problem in this respect. It makes sense to define constraints for testing those paths that loop through. This is done by combining paths from a certain number of loop runs into classes, which are considered to be sufficiently tested by selecting a test path of the class. The boundary-interior approach according to \TODO{Howden75} distinguishes test cases into 'boundary tests', where a loop is entered, but not iterated. The 'interior test' on the other hand enters the loop and also iterates it at least one time. A more general look at Howdens definition reveals the distinction into 'no loop execution', 'one-time loop execution' and 'multiple loop execution'. It shall be pointed out, that pre-checked loops remain untouched by cases of the first type.


	% In testing, it is assumed that a complete set of tests must test alternative paths through the top level of a program, alternative paths through loops, and alternative boundary tests of loops. A boundary test of a loop is a test which causes the loop to be entered but not iterated. An interior ?Control flow-oriented, structure-oriented tests

	% Alternatively, the structured path test 
	% \TODO{modified boundary interior ausm Ligges 120...123}

	\subsubsection{Modified boundary interior test technique}

	Following, a modified boundary interior test technique is suggested:

	\begin{enumerate}
		\item Requirement for test cases neglecting loops: All executable paths that do not enter rejecting loops and do not repeat non-rejecting loops must be tested.
		\item Requirements for test cases considering loops:
		\item For each rejecting loop, all executable partial paths are to be tested that
			\begin{itemize}
				\item execute the loop body exactly once and do not differ only in the iteration of nested loops.
				\item Paths that only have differences outside of the rejection loop under consideration do not have to be distinguished.
			\end{itemize}
			\begin{itemize}
				\item For each rejecting and each non-rejecting loop, all executable subpaths are to be tested that execute the loop body at least twice and the first two executions of the loop body differ not only in the iteration of nested loops.
				\item Paths that only have differences outside the loop under consideration or inside the loop from the third pass do not have to be differentiated.
			\end{itemize}
		\item The rules mentioned are to be applied separately for each loop.
		\item If branches are not tested, corresponding additional test cases are required.
	\end{enumerate}

	The first requirement ensures a path coverage test with the omission of loops. This can usually be done for reasonably designed modules with a reasonable amount of effort. The requirements listed furthermore essentially correspond to those of the boundary interior test for dealing with loops. By considering a single loop at a time and ignoring the control structures surrounding it and neglecting paths that result from nested loops, a reduction in the number of test cases is achieved. The last requirement ensures branch coverage regardless of the reachability of certain paths.

	The modified boundary interior test technique strives to reduce the test effort by considering the software to be tested in a modular way. For the areas outside of loops, test cases are required separately from the increase in complexity caused by loops. Each loop is considered individually, and nested loops are also ignored. However, this does not mean that each loop has to be tested individually. And finally, the branch coverage test is ensured by a corresponding explicit requirement.	
	% A relatively complicated section of a control flow graph is shown in Fig. 3-10. It represents a sequence of a selection, a non-rejecting loop, and a rejection loop. Nested within the rejection loop is a sequence of a selection and a rejection loop. In Tab. 3-20 the paths to be traversed for a modified boundary interior test are given as a sequence of the node numbers from Fig. 3-10. Alternatives are separated by a vertical bar; i.e. H. (n; | n,) means that exactly one of the alternatives n, or n, must be at this point of the path. A superscript indicates repetitions; i.e. H. (n;n,);i20 means that the sequence (n;n,) can appear at this position as often as you like and can also disappear. The relevant sections of the paths are highlighted in bold in Tab. 3-20. Figures 3-11 to 3-14 represent them graphically. It can be seen in the figures that it is possible to combine paths. Only the parts of the paths highlighted in bold are relevant for the test. The non-highlighted parts of the paths can be pronounced arbitrarily. It is therefore advisable to use these according to the requirements of other test paths. So e.g. For example, the partial paths according to Fig. 3-12 c and Fig. 3-12 d can be combined. This reduces the testing effort, since several required tests can be carried out with one test case. However, it is not guaranteed that this path, which repeats the non-rejecting loop and then executes the body of the rejecting loop exactly once, according to Fig. 3-12 d, can be executed at all. However, based on Figs. 3-11 to 3-14, it is easy to see that there are alternative possible combinations. The sub-test path according to Fig. 3-12 c can be combined with all sub-test paths of Fig. 3-12 d to Fig. 3-14 K. Such combinations, e.g. B. Fig. 3-12 c with Fig. 3-12 d, Fig. 3-13 h with Fig. 3-14 j and Fig. 3-13 i with Fig. 3-14 k, a reduction to eight test cases can be achieved will. If branches are not executed due to non-executable sub-test paths, additional test paths must be selected in order to achieve full branch coverage.


	When testing loops, also the maximum number of loop iterations shall be tested as well as exceeding the maximum number of iterations. In contrast to a small number of loop iterations, a large number of iterations is not necessarily considered appropriately by the boundary interior test, since interior tests can be aborted after the second loop iteration. Choosing a high number of repetitions is, however, entirely possible in interior tests. This is strongly recommend.

	The modified boundary interior test can very easily be generalized to a modified structured path test (with parameter k) by transferring the above requirements from k=2 to values greater than 2.
	% A comparative study/Howden 78a, b, c/ based on six programs in Algol, Cobol, PL/1, Fortran and PL360 with a total of 28 errors has resulted in a quota of 12 detected errors for the structured path test. This is a twofold higher success rate than the branch coverage test. 18 errors were detected using the path coverage test. While the branch coverage test led to the detection of all errors in only one of the six programs, all errors in three programs were detected with the help of the structured path test.

	Since the boundary interior test is a special case of the structured path test, a similar performance can be expected.


	% \subsection{application to RT / bare metal-Systems}
	% \subsection{start time}
	% \subsection{Deadline}
	% \subsection{execution time}
	% \subsection{Laxity}
	
	% \subsection{Reviews}
	
	% \subsection{Load/Fault tests}
	
	% \subsection{PM and ReqEng}
	
	% \subsection{V-Model?}

	% \section{Real-time and Reliability}
	% \subsection{soft/firm/hard}
	% \subsection{Jitter}
	% \subsection{Timing}
	% \subsubsection{Deadline}
	% \subsubsection{Laxity}
	% \subsubsection{Execution time}
	% \subsection{Load/Fault Hypothesis}

	% \section{Theory - CI/CD}
	% \subsection{CI/CD with open source on BareMetal}
	% \subsection{reliable USB-Connectivity}
	% \subsection{prepared for utilisation of complete Processor}
	% \section{Theory - Galvos}
	% \subsection{sensitive steering of Galvos}
